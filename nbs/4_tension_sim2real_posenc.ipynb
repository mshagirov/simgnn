{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GNN with Positional Encoding\n",
    "---\n",
    "> Graph neural network model for vertex dynamics and tension prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-19T04:56:35.240327Z",
     "start_time": "2021-11-19T04:56:35.218207Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-19T04:56:36.759617Z",
     "start_time": "2021-11-19T04:56:35.398099Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from shutil import rmtree\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import datetime\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "import networkx as nx\n",
    "from simgnn.datautils import load_array, load_graph\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "plt.style.use(['seaborn-paper', 'seaborn-ticks']) # use seaborn-talk for presentations\n",
    "%matplotlib inline\n",
    "# matplotlib.rcParams['figure.figsize'] = (10,10) # use larger for presentation\n",
    "# matplotlib.rcParams['font.size']= 16 # use 14 for presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-19T04:56:39.988209Z",
     "start_time": "2021-11-19T04:56:39.954160Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from simgnn.datasets import persistence_loss, VertexDynamics, HaraMovies, HaraAblation\n",
    "from simgnn.transforms import Pos2Vec, AddNoise_x, ScaleVelocity, ScaleTension, RecoilAsTension, TransformTension\n",
    "from simgnn.transforms import AppendReversedEdges, AppendEdgeNorm, AppendDiff_x, Reshape_x, AppendEdgeLen\n",
    "# from torch_geometric.utils import to_undirected as T_undir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-19T04:56:41.898565Z",
     "start_time": "2021-11-19T04:56:41.865371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaults:\n",
      " |-device: cuda\n",
      " |-dtype : torch.float32\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "dtype = torch.float32\n",
    "print(f'Defaults:\\n |-device: {device}\\n |-dtype : {dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_processed(data_paths):\n",
    "    '''\n",
    "    Delete folder if it is named `processed`.\n",
    "    \n",
    "    data_paths: PosixPath or a list of PosixPaths\n",
    "    '''\n",
    "    if type(data_paths) != list:\n",
    "        data_paths = [data_paths]\n",
    "    for dset in data_paths:\n",
    "        if dset.exists() and (dset.name=='processed'):\n",
    "            print('Clearing: ',str(dset))\n",
    "            rmtree(dset)\n",
    "        else:\n",
    "            print('Skipping: ',str(dset), f'{\"\" if dset.exists() else \"(does not exist)\"}')\n",
    "\n",
    "data_root = Path('simgnn_data')\n",
    "datasets = ['single_distr_sims/train', 'single_distr_sims/val',\n",
    "            'unimodal_wbasetens_sims/train', 'unimodal_wbasetens_sims/val',\n",
    "            'hara_movies',\n",
    "            'hara_ablation',\n",
    "            'hara_movies_as_sep_datasets/hara_seg001/',\n",
    "            'hara_movies_as_sep_datasets/hara_seg003/',\n",
    "            'hara_movies_as_sep_datasets/hara_seg005/' ]\n",
    "\n",
    "dataset_paths = [data_root / dset / 'processed' for dset in datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data, Stat-s, Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes for normalisation param-s:\n",
    "\n",
    "- Use `simgnn.transforms` to normalise real--Hara movies and ablation,  and simulated movies.\n",
    "- For ablation movies, convert to  HaraMovies length scale (pixels) and use same `l_av` as for HaraMovies dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "|Dataset| Avg. edge len.|Tension/Recoil| Transforms (Normalisation)|Notes|\n",
    "|:---:|:---:|---:|:---|:---:|\n",
    "|`single_distr_sims` | 1.08 a.u. | range = [0.00131, 4.26]; sd=0.637; median=0.305; mean=0.578; | `[Pos2Vec(scale=10*1.0)`, `ScaleVelocity(0.5*1.0)`, `ScaleTension(0.634,shift=0.6)]`| synthetic; w/o base tension |\n",
    "|`unimodal_wbasetens_sims` | 0.906 a.u. | range=[0.407, 5.47]; sd=0.729; median=1.48; mean=1.6| `[Pos2Vec(scale=10*1.0)`, `ScaleVelocity(0.5*1.0)`, `ScaleTension(0.634,shift=0.6)]`| synthetic; w/ non-zero base tension |\n",
    "|`HaraMovies`  | 26.32 pixels||`[Pos2Vec(scale=10*26.32)`, `ScaleVelocity(0.5*26.32)]`| tissue movies w/o force data|\n",
    "|`HaraAblation`|$26.32\\cdot\\frac{0.4}{0.21}$ pixels| range=[0.0239, 2.61]; sd=0.651; median=0.673; mean=0.826 | `[Pos2Vec(scale=10*50.13), ScaleVelocity(0.5*50.13)]`| short movies w/ recoil data; frame rate:dataset contains vertex positions only from every 10th frame |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **Normalisation param-s**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-19T04:38:21.352011Z",
     "start_time": "2021-11-19T04:38:21.324365Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Set features window size to --> 5\n",
      "> Set n for SMA_n --> 5\n",
      "> Input noise --> None\n"
     ]
    }
   ],
   "source": [
    "# Data normalisation param-s : sim=\"default\" for simulation data\n",
    "# Average edge lengths\n",
    "l_0 = {'sim':1.0, 'single_distr_sims':1.1, 'unimodal_wbasetens_sims':0.91, 'hara':26.32, 'abln':50.13}\n",
    "\n",
    "# Stat-s for raw tension/recoil values\n",
    "t_av = {'sim':0.6, 'single_distr_sims': 0.58, 'unimodal_wbasetens_sims': 1.6, 'abln': 0.83}\n",
    "t_sd = {'sim':0.63, 'single_distr_sims': 0.64, 'unimodal_wbasetens_sims':0.73 , 'abln': 0.65}\n",
    "\n",
    "# Input features\n",
    "window_size = 5\n",
    "print(f'> Set features window size to --> {window_size}')\n",
    "sma_lag_time = 4 # SMA_n : n=sma_lag_time+1\n",
    "print(f'> Set n for SMA_n --> {sma_lag_time+1}')\n",
    "\n",
    "pos_noise = None\n",
    "# pos_noise = torch.normal\n",
    "# noise_args= [0, 0.01] #[0,1]\n",
    "print(f'> Input noise --> {pos_noise}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input features and normalisation param-s (transforms)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-19T02:51:58.174970Z",
     "start_time": "2021-11-19T02:51:58.127408Z"
    }
   },
   "outputs": [],
   "source": [
    "# Simulation datasets\n",
    "# Simulation dataset\n",
    "# Normalisation (**no input noise**)\n",
    "Tnrm = {k: [Pos2Vec(scale=l_0['sim']), ScaleVelocity(l_0['sim']), ScaleTension(t_sd[k], shift=t_av[k])]\n",
    "        for k in ['sim', 'single_distr_sims', 'unimodal_wbasetens_sims']}\n",
    "\n",
    "\n",
    "# Hara ablation dataset\n",
    "Tnrm['abln'] = [Pos2Vec(scale=l_0['abln']), ScaleVelocity(l_0['abln']), RecoilAsTension(), \n",
    "                ScaleTension(t_sd['abln'], shift=t_av['abln'])]\n",
    "\n",
    "# Hara movie dataset\n",
    "Tnrm['hara'] = [Pos2Vec(scale=l_0['hara']), ScaleVelocity(l_0['hara'])]\n",
    "\n",
    "\n",
    "# Node and Edge Feature Transforms\n",
    "# Convert to undirected graph and append \"edge features\"\n",
    "for k in Tnrm:\n",
    "    # to undirected graph: add e_ji for all e_ij\n",
    "    Tnrm[k].append(AppendReversedEdges())\n",
    "    \n",
    "    # Add edge features\n",
    "    Tnrm[k].append(AppendDiff_x()) # DiffX feature\n",
    "    \n",
    "    # miscellaneous variables\n",
    "    Tnrm[k].append(AppendEdgeLen(norm=True, scale=l_0[k])) # for edge masks (not an edge feature)\n",
    "    \n",
    "    # flatten  node features\n",
    "    Tnrm[k].append(Reshape_x((-1,window_size*2)))\n",
    "    \n",
    "    Tnrm[k] = T.Compose(Tnrm[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose([\n",
       "    Pos2Vec(norm=True, scale=1.0, cat=False, pos_noise=None, noise_args=[], noise_kwargs={}),\n",
       "    ScaleVelocity(scale=1.0),\n",
       "    ScaleTension(scale=0.73, shift=1.6),\n",
       "    AppendReversedEdges(reverse_attr=True, reverse_tension=False, edge_id=True),\n",
       "    AppendDiff_x(norm=True),\n",
       "    AppendEdgeLen(keep_dir=False, aggr_edge_id=True, use_edge_attr=False, norm=True, scale=0.91),\n",
       "    Reshape_x((-1, 10)),\n",
       "])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tnrm['unimodal_wbasetens_sims']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if one of these is changed: `w`, SMA `lag` time, delete the old *processed* dataset files (e.g. with old `w`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping:  simgnn_data/single_distr_sims/train/processed (does not exist)\n",
      "Skipping:  simgnn_data/single_distr_sims/val/processed (does not exist)\n",
      "Skipping:  simgnn_data/unimodal_wbasetens_sims/train/processed (does not exist)\n",
      "Skipping:  simgnn_data/unimodal_wbasetens_sims/val/processed (does not exist)\n",
      "Skipping:  simgnn_data/hara_movies/processed (does not exist)\n",
      "Skipping:  simgnn_data/hara_ablation/processed (does not exist)\n",
      "Skipping:  simgnn_data/hara_movies_as_sep_datasets/hara_seg001/processed (does not exist)\n",
      "Skipping:  simgnn_data/hara_movies_as_sep_datasets/hara_seg003/processed (does not exist)\n",
      "Skipping:  simgnn_data/hara_movies_as_sep_datasets/hara_seg005/processed (does not exist)\n"
     ]
    }
   ],
   "source": [
    "clear_processed(dataset_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T14:58:32.713277Z",
     "start_time": "2021-10-25T14:58:30.420983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n",
      "Processing...\n",
      "Done!\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Simulation datasets\n",
    "# rand base contractility\n",
    "sim1 = VertexDynamics('./simgnn_data/unimodal_wbasetens_sims/train/', window_size=window_size,\n",
    "                      transform=Tnrm['unimodal_wbasetens_sims']\n",
    "                     )\n",
    "sim1_val = VertexDynamics('./simgnn_data/unimodal_wbasetens_sims/val/', window_size=window_size,\n",
    "                          transform=Tnrm['unimodal_wbasetens_sims']\n",
    "                         )\n",
    "\n",
    "# w/o base contractility\n",
    "sim2_val = VertexDynamics('./simgnn_data/single_distr_sims/val/', window_size=window_size,\n",
    "                          transform=Tnrm['single_distr_sims']\n",
    "                         )\n",
    "\n",
    "\n",
    "datasets_dict = {'train': sim1,\n",
    "                 'val': sim1_val[:len(sim1_val)//2]+sim1_val[len(sim1_val)//2:-1],\n",
    "                 'val2': sim2_val[:len(sim2_val)//2]+sim1_val[len(sim2_val)//2:-1],\n",
    "                }\n",
    "dataset_legend={'train': 'Train(sim1)',\n",
    "                'val': r'Supervised ($\\mathcal{D}_{sim1}$)',\n",
    "                'val2': r'Sim2sim ($\\mathcal{D}_{sim2})$',\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "\n",
    "Reading\n",
    "1. [Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains](https://bmild.github.io/fourfeat/index.html)\n",
    "    - colab nb https://colab.research.google.com/github/tancik/fourier-feature-networks/blob/master/Demo.ipynb\n",
    "1. [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "1. Related:\n",
    "    - [NeRF](https://www.matthewtancik.com/nerf)\n",
    "    - [Multi-scale NeRF](https://jonbarron.info/mipnerf/)\n",
    "    - [Learned initialisation](https://www.matthewtancik.com/learnit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T06:39:34.589668Z",
     "start_time": "2021-10-07T06:39:34.556697Z"
    }
   },
   "outputs": [],
   "source": [
    "# k = 'val2'\n",
    "# t = 15\n",
    "# print('{}\\n{} : {}\\nsize:{}\\n\\nFrame {}:\\n{}'.format(dataset_legend[k],k,datasets_dict[k],len(datasets_dict[k]),t,datasets_dict[k][0]))\n",
    "# plt.figure(figsize=[2.5,2.5])\n",
    "# nx.draw(to_networkx(datasets_dict[k][t]),pos=dict(enumerate(datasets_dict[k][t].pos.numpy())), node_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "211.796875px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
