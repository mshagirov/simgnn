{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Network\n",
    "---\n",
    "> Graph neural network model for vertex dynamics and tension prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To-Do**ðŸ‘·ðŸš§\n",
    "\n",
    "- *Training loop*:\n",
    "    - [ ] Training loop w/ validation set error monitoring, and best model saving\n",
    "    - [ ] Combine `Message` and `AggregateUpdate` into a graph layer `GraphBlock` (a more general block/model that can be composed into a deep residual network). \"AddGN\" block, w/ `AddGN(x) = f(x)+x` form (in fact, where it's possible make all blocks with this form).\n",
    "- [ ] *Prediction stage*: read \\{test, val, train\\} data and predict w/ saving.\n",
    "- [ ] Ablation dataset (*real*).\n",
    "- [ ] Larger simul-n dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DOING**ðŸ› \n",
    "\n",
    "1-val data `dataset` obj, and dataloder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Node-to-Cell Encoding/Pooling Layer**:\n",
    "1. Initiate node-to-cell edge attr-s as (source) node attr-s `x[node2cell_index[0]]`.\n",
    "1. Compute node-to-cell edge attr-s using MLP: `e_n2c = MLP( x[node2cell_index[0]] )`\n",
    "1. Aggregate node-to-cell edge attr-s as cell attr-s : `x_cell = Aggregate(e_n2c)`\n",
    "1. Compute new cell attr-s using (encodes `x_cell` into cell attr-s) : `h_cell = MLP_Cell_encoder( x_cell )`\n",
    "\n",
    "```python\n",
    "n2c_model = mlp(...) # \"message\", just node-wise MLP\n",
    "cell_aggr = Aggregate()\n",
    "cell_enc = mlp(...)\n",
    "\n",
    "e_n2c = n2c_model(data.x)[data.node2cell_index[0]]\n",
    "x_cell = cell_aggr(data.cell_pressures.size(0), data.node2cell_index, e_n2c)\n",
    "h_cell = cell_enc(x_cell)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Node features** : velocities from previous frames (~ 5 frames--> Alvaro Sanchez-Gonzalez, *et al.* 2020 \\[ASG2020\\])\n",
    "- **Edge features** : can use edge directions (optional, might help to speed up training)\n",
    "- **Current position** : technically a node feature, and needs to be normalized but not processed by the network.\n",
    "- Positions (Cartesian) to polar edge attributes: `transforms.Polar` \\[[link](https://pytorch-geometric.readthedocs.io/en/latest/modules/transforms.html#torch_geometric.transforms.Polar)\\], I can also implement or use the `transforms.Cartesian` which computes direction vectors (position pairs -to- normalized direction vectors).\n",
    "- need transform for velocity noise (use src from example transforms above and ASG2020 paper)\n",
    "- Train-g movie: max edge length ~ 3.25 a.u."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples**:\n",
    "- General \"Message Passing\" schemes: a nice example for composite graph layer â€“\"meta layer\" consisting of \"edge\", \"node\" and \"global\" layers [link](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.meta.MetaLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Adding noise from M steps**: Sum of M normal rand. var-s results in normal var. w/ variance M and s.t.d.=sqrt(M):\n",
    "```python\n",
    "x = np.random.normal(size=(5,1000))\n",
    "y = x.sum(axis=0)\n",
    "z = np.random.normal(size=(1,1000))*np.sqrt(5)\n",
    "plt.hist(x.ravel(),bins=50,label='x',density=True)\n",
    "plt.hist(y        ,bins=50,label='y',density=True)\n",
    "plt.hist(z.ravel(),bins=50,label='z',density=True,alpha=.5)\n",
    "plt.legend();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data pre-processing and normalisation:\n",
    "    - Simulation datasets:\n",
    "```python\n",
    "Tnorm = T.Compose([Pos2Vec(scale=10*0.857) ,\n",
    "                   ScaleVelocity(0.5*0.857), \n",
    "                   ScaleTension(5,shift=1.45),\n",
    "                   ScalePressure(3, shift=1.0)])\n",
    "```\n",
    "\n",
    "    - Hara movies:\n",
    "```python\n",
    "T_hara_movie_norm = T.Compose([Pos2Vec(scale=10*26.32) ,\n",
    "                           ScaleVelocity(0.5*26.32)])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T06:57:46.793525Z",
     "start_time": "2021-04-27T06:57:46.770451Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T06:57:50.839642Z",
     "start_time": "2021-04-27T06:57:48.355892Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from os import path\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "import networkx as nx\n",
    "from simgnn.datautils import load_array, load_graph\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (10,10) # use larger for presentation\n",
    "matplotlib.rcParams['font.size']= 14 # use 14 for presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T06:57:59.449305Z",
     "start_time": "2021-04-27T06:57:59.413636Z"
    }
   },
   "outputs": [],
   "source": [
    "from simgnn.datasets import VertexDynamics, HaraMovies\n",
    "from simgnn.nn import mlp, Message, AggregateUpdate, Aggregate\n",
    "from simgnn.transforms import Pos2Vec, ScaleVelocity, ScaleTension, ScalePressure\n",
    "# from torch_geometric.utils import to_undirected as T_undir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T06:58:00.701847Z",
     "start_time": "2021-04-27T06:58:00.663611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaults:\n",
      " |-device: cpu\n",
      " |-dtype : torch.float32\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "dtype = torch.float32\n",
    "print(f'Defaults:\\n |-device: {device}\\n |-dtype : {dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hara Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T06:58:10.332417Z",
     "start_time": "2021-04-27T06:58:10.160636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_Length.npy     edges_index.npy     node2cell_index.npy vtx_pos.npy\r\n"
     ]
    }
   ],
   "source": [
    "!ls 'simgnn_data/hara_movies/raw/Seg_001/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T06:58:10.531550Z",
     "start_time": "2021-04-27T06:58:10.417328Z"
    }
   },
   "outputs": [],
   "source": [
    "T_hara_movie_norm = T.Compose([Pos2Vec(scale=10*26.32) ,\n",
    "                           ScaleVelocity(0.5*26.32)]) # data normalisation\n",
    "hara_movs = HaraMovies('simgnn_data/hara_movies/', transform=T_hara_movie_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T06:58:10.918749Z",
     "start_time": "2021-04-27T06:58:10.890973Z"
    }
   },
   "outputs": [],
   "source": [
    "# mov_i = hara_movs[0]\n",
    "# nx.draw(to_networkx(T.to_undirected.ToUndirected()(mov_i), to_undirected=True),pos=dict(enumerate(mov_i.pos.numpy())), node_size=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T06:58:11.231595Z",
     "start_time": "2021-04-27T06:58:11.200441Z"
    }
   },
   "outputs": [],
   "source": [
    "# nx.draw(to_networkx(batch, to_undirected=False),pos=dict(enumerate(batch.pos.numpy())), node_size=60)\n",
    "\n",
    "# for c_i in batch.cell2node_index[0].unique():\n",
    "#     c_vx = batch.pos[batch.cell2node_index[1][batch.cell2node_index[0]==c_i]]\n",
    "#     c_x = c_vx.mean(axis=0)\n",
    "#     for c_vx_i in c_vx:\n",
    "#         plt.plot([c_vx_i[0], c_x[0]], [c_vx_i[1], c_x[1]], 'b', lw=3, alpha=.3)\n",
    "#     plt.plot(c_x[0],c_x[1], 'ro', ms=5)\n",
    "# plt.plot(batch.pos[:,0],batch.pos[:,1],'yo',ms=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T06:58:11.728221Z",
     "start_time": "2021-04-27T06:58:11.671674Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CellData(cell2node_index=[2, 492], edge_attr=[225, 2], edge_index=[2, 225], node2cell_index=[2, 492], pos=[164, 2], x=[164, 5, 2], y=[164, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hara_movs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training w/ Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CellData` prop-s (graph data objects):\n",
    "- `x` : `(#nodes, WindowWidth, 2)` *node features*\n",
    "- `y` : `(#nodes, 2)` *node targets (velocities)*.\n",
    "- `pos` : `(#nodes, 2)` *node positions*.\n",
    "- `edge_attr` : `(#edges, 2)` or `(#edges, #edge_features)` *edge features  (relative Cartesian positions of connected nodes)*.\n",
    "- `edge_index` : `(2, #edges)` *edge indices*.\n",
    "- `edge_tensions` : `(#edges,)` *edge targets (line tensions)*.\n",
    "- `node2cell_index` : `(2, #cell2node_edges)`, `node2cell`-> *first row is node indices and second row is cell indices;\n",
    "- `cell2node_index` : `(2, #cell2node_edges)`, `cell2node`-> *first row is cell indices and second row is node indices*.\n",
    "- `cell_pressures` : `(#cells,)` *cell targets (cell pressures)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T06:58:12.938861Z",
     "start_time": "2021-04-27T06:58:12.903721Z"
    }
   },
   "outputs": [],
   "source": [
    "# test = VertexDynamics('../../../dataDIR/simgnn_data/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T17:55:14.802172Z",
     "start_time": "2021-04-27T17:55:14.757678Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalisation: for simulated data\n",
    "Tnorm = T.Compose([Pos2Vec(scale=10*0.857) ,\n",
    "                   ScaleVelocity(0.5*0.857), \n",
    "                   ScaleTension(5,shift=1.45),\n",
    "                   ScalePressure(3, shift=1.0)])\n",
    "# training dataset\n",
    "vtxdata = VertexDynamics('../../../dataDIR/simgnn_data/train/', transform=Tnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T17:55:15.409068Z",
     "start_time": "2021-04-27T17:55:15.368444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VertexDynamics(95) \n",
      " CellData(cell2node_index=[2, 600], cell_pressures=[100], edge_attr=[339, 2], edge_index=[2, 339], edge_tensions=[339], node2cell_index=[2, 600], pos=[240, 2], x=[240, 5, 2], y=[240, 2])\n"
     ]
    }
   ],
   "source": [
    "print('',vtxdata,'\\n',vtxdata[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T17:55:15.880191Z",
     "start_time": "2021-04-27T17:55:15.853407Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CellData(cell2node_index=[2, 600], cell_pressures=[100], edge_attr=[339, 2], edge_index=[2, 339], edge_tensions=[339], node2cell_index=[2, 600], pos=[240, 2], x=[240, 5, 2], y=[240, 2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = vtxdata[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T17:55:17.612367Z",
     "start_time": "2021-04-27T17:55:16.392431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Cell Pressures\n",
      "\trange: [-0.178 0.235]; s.d.: 0.0666 || median: 0.00759; mean: 0.0098;\n",
      "> Edge Tensions\n",
      "\trange: [-0.185 0.292]; s.d.: 0.0662 || median: -0.022; mean: -0.00983;\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "c_pres = torch.cat([d.cell_pressures for d in vtxdata],0)\n",
    "e_tens = torch.cat([d.edge_tensions for d in vtxdata],0)\n",
    "\n",
    "print(f'> Cell Pressures\\n\\trange: [{c_pres.min():4.3g} {c_pres.max():4.3g}]; s.d.: {c_pres.std():4.3g} ||'+\n",
    "      f' median: {c_pres.median():4.3g}; mean: {c_pres.mean():4.3g};')\n",
    "print(f'> Edge Tensions\\n\\trange: [{e_tens.min():4.3g} {e_tens.max():4.3g}]; s.d.: {e_tens.std():4.3g} ||'+\n",
    "      f' median: {e_tens.median():4.3g}; mean: {e_tens.mean():4.3g};')\n",
    "\n",
    "plt.figure(figsize=[10,3])\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "ax1.hist(c_pres.view(-1,).numpy(),bins=30,color='salmon');\n",
    "ax1.set_title('Cell Pressures')\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "ax2.hist(e_tens.view(-1,).numpy(),bins=30,color='c');\n",
    "ax2.set_title('Edge Tensions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-28T17:57:21.581398Z",
     "start_time": "2021-02-28T17:57:21.579353Z"
    }
   },
   "outputs": [],
   "source": [
    "# data.is_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-28T17:57:18.144326Z",
     "start_time": "2021-02-28T17:57:18.141230Z"
    }
   },
   "outputs": [],
   "source": [
    "# nx.draw(to_networkx(data),pos=dict(enumerate(data.pos.numpy())), node_size=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T13:02:38.730513Z",
     "start_time": "2021-02-08T13:02:38.665269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(batch=[480], cell2node_index=[2, 1200], cell_pressures=[200], cell_pressures_batch=[200], edge_attr=[678, 2], edge_index=[2, 678], edge_index_batch=[678], edge_tensions=[678], node2cell_index=[2, 1200], pos=[480, 2], x=[480, 5, 2], y=[480, 2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in order to track the batch id for var-s add its key to \"follow_batch\":\n",
    "loader = DataLoader(vtxdata, batch_size=2,follow_batch=['cell_pressures','edge_index'])\n",
    "# this tracks batch id for \"cell_pressures_batch\" and \"edge_index_batch\" in addition to node batch ids\n",
    "batch = next(iter(loader))\n",
    "# nx.draw( to_networkx(\n",
    "#     CellData(num_nodes = torch.sum(batch.batch==0).item(),\n",
    "#              edge_index = batch.edge_index[:,batch.edge_index_batch==0])),\n",
    "#     pos=dict(enumerate(batch.pos[batch.batch==0].numpy())),\n",
    "#     node_size=60, node_color='r',edge_color='r')\n",
    "# nx.draw( to_networkx(\n",
    "#     CellData(num_nodes = torch.sum(batch.batch==1).item(),\n",
    "#              edge_index = batch.edge_index[:,batch.edge_index_batch==1]-240)),\n",
    "#     pos=dict(enumerate(batch.pos[batch.batch==1].numpy() +1.5)),\n",
    "#     node_size=60)\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# useful functions for model training and saving, etc.\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "def train_model(model,\n",
    "                optimizer,\n",
    "                data_loaders,\n",
    "                num_epochs = 5,\n",
    "                loss_func = torch.nn.CrossEntropyLoss(),\n",
    "                device = torch.device('cpu'),\n",
    "                scheduler = None,\n",
    "                return_best = False,\n",
    "               classifier=None):\n",
    "    '''\n",
    "    docs\n",
    "    '''\n",
    "    # model states/modes\n",
    "    model_states = ['train', 'val']\n",
    "    training_model=model\n",
    "    if classifier!=None:\n",
    "        training_model=classifier # transfer learning\n",
    "    \n",
    "    curve_data = {'trainLosses':[],\n",
    "                 'trainAccs':[],\n",
    "                 'valLosses':[],\n",
    "                 'valAccs':[],\n",
    "                 'total_epochs':num_epochs}\n",
    "    \n",
    "    time_start = time.time()\n",
    "    if return_best:\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1} ---', end=' ')\n",
    "        \n",
    "        # set model state depending on training/eval stage\n",
    "        for state in model_states:\n",
    "            if state == 'train':\n",
    "                training_model.train()  # Set model to training mode\n",
    "            else:\n",
    "                training_model.eval()   # Set model to evaluation mode\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            for samples in data_loaders[state]:\n",
    "                # input HxW depend on transform function(s), 3 Channels\n",
    "                inputs = samples['image'].to(device)\n",
    "                # labels \\in [0, 1, 2]\n",
    "                labels = samples['label'].to(device)          \n",
    "                \n",
    "                # set grad accumulator to zero\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(state == 'train'):\n",
    "                    # grad tracking is disabled in \"eval\" mode\n",
    "                    outputs = model(inputs) # output:(batch, #classes)\n",
    "                    _, preds = torch.max(outputs, 1) # labels:(batch,)\n",
    "                    loss = loss_func(outputs, labels) #<-torch.nn.CrossEntropyLoss\n",
    "                    \n",
    "                    if state == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0) # weighted loss\n",
    "                running_corrects += torch.sum(preds == labels.detach() )\n",
    "            \n",
    "                # apply LR schedule\n",
    "                if state == 'train' and scheduler!=None:\n",
    "                    scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / len(data_loaders[state].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(data_loaders[state].dataset)\n",
    "            \n",
    "            curve_data[f'{state}Losses'].append(epoch_loss)\n",
    "            curve_data[f'{state}Accs'].append(epoch_acc)\n",
    "            \n",
    "            print(f'{state} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}',end=' || ')\n",
    "            \n",
    "            # deep copy the model\n",
    "            if state == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                if return_best:\n",
    "                    # keep best weights to return\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        print(f'{time.time() - time_start:.0f}s')\n",
    "    time_elapsed = time.time() - time_start\n",
    "    print(f'Training complete in {time_elapsed//60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f} (return best:{return_best})')\n",
    "    \n",
    "    if return_best:\n",
    "        # load best model weights\n",
    "        model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, curve_data\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_check(data_loaders, model,device=torch.device('cpu')):\n",
    "    '''\n",
    "    Run prediction on datasets using dataloader\n",
    "    - data_loaders: data loaders (dict of torch.utils.data.DataLoader objects) with\n",
    "                    keys 'train' and 'val', for training and validation data loaders respectively.\n",
    "                    ! DISABLE SHUFFLING in both datasets in order to preserve order of IDs\n",
    "    - model: model used for prediction\n",
    "    - device: device, e.g. \"torch.device('cuda')\"\n",
    "    '''\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    losses = {'train':0, 'val': 0}\n",
    "    accuracies = {'train':0, 'val':0}\n",
    "    pred_labels = {'train':[],'val':[]}\n",
    "    \n",
    "    model.eval()\n",
    "    default_device = next(model.parameters()).device\n",
    "    model.to(device)\n",
    "    for loader_type in data_loaders:\n",
    "        print(f'Loading: {loader_type}')\n",
    "        for samples in data_loaders[loader_type]:\n",
    "            inputs = samples['image'].to(device) # input images\n",
    "            labels = samples['label'].to(device) # labels \\in [0, 1, 2]       \n",
    "            # predict\n",
    "            outputs = model(inputs) # output:(batch, #classes)\n",
    "            _, preds = torch.max(outputs, 1) # labels:(batch,)\n",
    "            preds = preds.cpu()\n",
    "            loss = loss_func(outputs, labels) #<-torch.nn.CrossEntropyLoss\n",
    "            losses[loader_type] += loss.item() * inputs.size(0) # weighted loss\n",
    "            accuracies[loader_type] += torch.sum(preds == labels.cpu() )\n",
    "            pred_labels[loader_type].extend(preds.tolist())\n",
    "            \n",
    "        losses[loader_type] = losses[loader_type] / len(data_loaders[loader_type].dataset)\n",
    "        accuracies[loader_type] = accuracies[loader_type] / len(data_loaders[loader_type].dataset)\n",
    "    print('Losses:',losses)\n",
    "    print('Accuracies:', accuracies)\n",
    "    \n",
    "    model.to(default_device)\n",
    "    return losses, accuracies, pred_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_test(root_path, model, transform, batch_size=4, device=torch.device('cpu')):\n",
    "    '''Run prediction on test images.\n",
    "    - root_path: path to the folder with test images\n",
    "    - model: model used for prediction \n",
    "    - batch_size: batch size for processing\n",
    "    - device: device, e.g. \"torch.device('cuda')\"\n",
    "    '''\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    # list of test image files\n",
    "    test_image_names= [path.split(imgname)[-1] for imgname in glob.glob(path.join(root_path,'*.png'))]\n",
    "    # sort according to image ID number\n",
    "    test_image_names.sort(key=lambda x: int(x.split('.')[0]))\n",
    "    ID = [int(imgname.split('.')[0]) for imgname in test_image_names]\n",
    "    N_samples = len(test_image_names)\n",
    "    print(f'Found {N_samples} images in test dataset folder: {path.join(*path.split(root_path)[:-1])}'+\n",
    "          f'\\n\\\"{test_image_names[0]}\\\"\\n\\\"{test_image_names[1]}\\\"\\n\\\"{test_image_names[2]}\\\"\\n. . .\\n'+\n",
    "          f'\\\"{test_image_names[-3]}\\\"\\n\\\"{test_image_names[-2]}\\\"\\n\\\"{test_image_names[-1]}\\\"]\\n')\n",
    "    \n",
    "    # iter over batches\n",
    "    pred_labels = []\n",
    "    N_batches = N_samples//batch_size + (1 if N_samples%batch_size else 0)\n",
    "    print(f'Processing {N_batches} test batches in total (batch_size={batch_size}).')\n",
    "    for b in range(N_batches):\n",
    "        last_idx = min([b*batch_size+batch_size,N_samples])\n",
    "        # read and transform images\n",
    "        img_batch = torch.stack([transform( imread(path.join(root_path,imgname)) )\n",
    "                     for imgname in test_image_names[b*batch_size:last_idx]],dim=0)\n",
    "        img_batch = img_batch.to(device)\n",
    "        # predict\n",
    "        outputs = model(img_batch) # output:(batch, #classes)\n",
    "        _, preds = torch.max(outputs, 1) # labels:(batch,)\n",
    "        preds = preds.cpu()\n",
    "        pred_labels.extend(preds.tolist())\n",
    "    print('Done.')\n",
    "    return {'ID': ID, 'Label': pred_labels}\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "174.796875px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
