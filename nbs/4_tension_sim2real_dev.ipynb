{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Network\n",
    "---\n",
    "> Graph neural network model for vertex dynamics and tension prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To-Do**ðŸ‘·ðŸš§\n",
    "- need a func-n w/ a **rollout error**,\n",
    "- convert vel-y error to **position error**, e.g. \"speed\"+\"direction\"(angle/dot product etc.)\n",
    "- *Training loop*:\n",
    "    - [ ] Combine `Message` and `AggregateUpdate` into a graph layer `GraphBlock`\n",
    "- [ ] Write *prediction stage*: read \\{test, val, train\\} data for rollout error measurements.\n",
    "- [ ] Experiments (**save all models** w/ backups; will need to test on the real tensions)<br>\n",
    "*Params*: edge attrib-s, cell layer, arch(skip con-s), input noise (for long term prediction). *Errors*: 1-step, and rollout error (check after training at test time).\n",
    "    1. Experiment with GN arch-s w/ *residual* (w/ skip) and *non-residual* architectures.\n",
    "    1. *Edge directions experiment*: train w/ and w/o edge dir-s, do edge dir-s help to speed up training?\n",
    "    1. *Cell layer*: does having dedicated cell processing layer help to increase the accuracy?\n",
    "    1. For best (resnet or non-resnet) run *number of layers and dim-n sizes*. Try deep nets, how does accuracy change with increasing the depth of the net?\n",
    "    1. If resnet is better: try w/and w/o edge dir-s and \"cell layers\".\n",
    "    1. Number of previous velocities (window size).\n",
    "    1. Rollout experiment 1â€” *input noise*: according to Sanchez-Gonzalez, *et al.* \\[ASG2020\\], Brownian noise improves rollout accuracies (long term accuracy of the whole movie, and/or prediction stability/robustness).\n",
    "    1. Rollout experiment 2â€” *rollout training*: train directly on rollout, i.e. use 5- and 10-step loss instead of a 1-step loss for training (slower training).\n",
    "    1. Optional:\n",
    "        - compare MLP vs CONV layers for message passing.\n",
    "        - try with dynamic graphs (construct graphs on the fly based on relative positions, and use cell edges and cell attrib only for queries on `Y_edge`, `Y_cell`).\n",
    "- [ ] Ablation dataset (*real*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**DOING**ðŸ› \n",
    "1. Construct graph net without skip connections.\n",
    "1. Combine Message and AggregateUpdate into a graph layer GraphNet (GN) block, a more general block, that can be composed into a deep residual network. \"AddGN\" block, w/ `AddGN(x) = f(x)+x` form (in fact, where it's possible make all blocks with this form.\n",
    "1. Construct residual net out of GNs.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Examples**:\n",
    "    - General \"Message Passing\" schemes: a nice example for composite graph layer â€“\"meta layer\" consisting of \"edge\", \"node\" and \"global\" layers [link](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.meta.MetaLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Node-to-Cell Encoding/Pooling Layer**:\n",
    "1. Initiate node-to-cell edge attr-s as (source) node attr-s `x[node2cell_index[0]]`.\n",
    "1. Compute node-to-cell edge attr-s using MLP: `e_n2c = MLP( x[node2cell_index[0]] )`\n",
    "1. Aggregate node-to-cell edge attr-s as cell attr-s : `x_cell = Aggregate(e_n2c)`\n",
    "1. Compute new cell attr-s using (encodes `x_cell` into cell attr-s) : `h_cell = MLP_Cell_encoder( x_cell )`\n",
    "\n",
    "```python\n",
    "n2c_model = mlp(...) # \"message\", just node-wise MLP\n",
    "cell_aggr = Aggregate()\n",
    "cell_enc = mlp(...)\n",
    "\n",
    "e_n2c = n2c_model(data.x)[data.node2cell_index[0]]\n",
    "x_cell = cell_aggr(data.cell_pressures.size(0), data.node2cell_index, e_n2c)\n",
    "h_cell = cell_enc(x_cell)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **Adding noise from M steps**: Sum of M normal rand. var-s results in normal var. w/ variance M and s.t.d.=sqrt(M):\n",
    "```python\n",
    "x = np.random.normal(size=(5,1000))\n",
    "y = x.sum(axis=0)\n",
    "z = np.random.normal(size=(1,1000))*np.sqrt(5)\n",
    "plt.hist(x.ravel(),bins=50,label='x',density=True)\n",
    "plt.hist(y        ,bins=50,label='y',density=True)\n",
    "plt.hist(z.ravel(),bins=50,label='z',density=True,alpha=.5)\n",
    "plt.legend();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T15:59:44.819869Z",
     "start_time": "2021-09-20T15:59:44.794557Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T15:59:47.060876Z",
     "start_time": "2021-09-20T15:59:45.469607Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from os import path\n",
    "import datetime\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "import networkx as nx\n",
    "from simgnn.datautils import load_array, load_graph\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (10,10) # use larger for presentation\n",
    "matplotlib.rcParams['font.size']= 16 # use 14 for presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T15:59:47.856921Z",
     "start_time": "2021-09-20T15:59:47.806590Z"
    }
   },
   "outputs": [],
   "source": [
    "from simgnn.datasets import persistence_loss, VertexDynamics, HaraMovies, HaraAblation\n",
    "from simgnn.transforms import Pos2Vec, ScaleVelocity, ScaleTension, ScalePressure, Reshape_x, TransformTension, RecoilAsTension\n",
    "# from torch_geometric.utils import to_undirected as T_undir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T15:59:48.616288Z",
     "start_time": "2021-09-20T15:59:48.579113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaults:\n",
      " |-device: cpu\n",
      " |-dtype : torch.float32\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "dtype = torch.float32\n",
    "print(f'Defaults:\\n |-device: {device}\\n |-dtype : {dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data, Stat-s, Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes for normalisation param-s:\n",
    "\n",
    "- Use `simgnn.transforms` to normalise real--Hara movies and ablation,  and simulated movies. Use same normalisation constants for all simulated movies.\n",
    "- For ablation movies, convert to  HaraMovies length scale (pixels) and use same `l_av` as for HaraMovies dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "|Dataset| Avg. edge len.|Tension/Recoil| Transforms (Normalisation)|Notes|\n",
    "|:---:|---:|---:|:---|:---:|\n",
    "|`single_distr_sims` | 1.08 a.u. | range=[0.00131, 4.26]; sd=0.637; median=0.305; mean=0.578; | `[Pos2Vec(scale=10*1.0)`, `ScaleVelocity(0.5*1.0)`, `ScaleTension(0.634,shift=0.6)]`| synthetic; w/o base tension |\n",
    "|`unimodal_wbasetens_sims` | 0.906 a.u. | range=[0.407, 5.47]; sd=0.729; median=1.48; mean=1.6| `[Pos2Vec(scale=10*1.0)`, `ScaleVelocity(0.5*1.0)`, `ScaleTension(0.634,shift=0.6)]`| synthetic; w/ non-zero base tension |\n",
    "|`HaraMovies`  | 26.32 pixels||`[Pos2Vec(scale=10*26.32)`, `ScaleVelocity(0.5*26.32)]`| tissue movies w/o force data|\n",
    "|`HaraAblation`|$26.32\\cdot\\frac{0.4}{0.21}$ pixels| range=[0.0239, 2.61]; sd=0.651; median=0.673; mean=0.826 | `[Pos2Vec(scale=10*50.13), ScaleVelocity(0.5*50.13)]`| short movies w/ recoil data; frame rate:dataset contains vertex positions only from every 10th frame |\n",
    "\n",
    "- Logarithm of tension: ln(Tension/Recoil {raw values})\n",
    "```\n",
    "Single_distr_sims:        range: [-6.63 1.45]; s.d.: 1.22 || median: -1.19; mean: -1.2;\n",
    "Unimodal_wbasetens_sims : range: [-0.9  1.7]; s.d.: 0.451 || median: 0.39; mean: 0.371;\n",
    "Hara ablation:            range: [-3.73 0.958]; s.d.:0.962 || median:-0.396; mean:-0.547;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **Normalisation param-s**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T15:59:51.012204Z",
     "start_time": "2021-09-20T15:59:50.975513Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data normalisation param-s : sim=\"default\" for simulation data\n",
    "# Average edge lengths\n",
    "l_0 = {'sim':1.0, 'single_distr_sims':1.1, 'unimodal_wbasetens_sims':0.91, 'hara':26.32, 'abln':50.13}\n",
    "\n",
    "# Stat-s for raw tension/recoil values\n",
    "t_av = {'sim':0.6, 'single_distr_sims': 0.58, 'unimodal_wbasetens_sims': 1.6, 'abln': 0.83}\n",
    "t_sd = {'sim':0.63, 'single_distr_sims': 0.64, 'unimodal_wbasetens_sims':0.73 , 'abln': 0.65}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T15:59:53.593980Z",
     "start_time": "2021-09-20T15:59:52.591203Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm -dr simgnn_data/single_distr_sims/train/processed\n",
    "!rm -dr simgnn_data/single_distr_sims/val/processed\n",
    "!rm -dr simgnn_data/unimodal_wbasetens_sims/train/processed\n",
    "!rm -dr simgnn_data/unimodal_wbasetens_sims/val/processed\n",
    "\n",
    "!rm -dr simgnn_data/hara_movies/processed/\n",
    "!rm -dr simgnn_data/hara_ablation/processed/\n",
    "\n",
    "# !rm -dr simgnn_data/hara_movies_as_sep_datasets/hara_seg001/processed/\n",
    "# !rm -dr simgnn_data/hara_movies_as_sep_datasets/hara_seg003/processed/\n",
    "# !rm -dr simgnn_data/hara_movies_as_sep_datasets/hara_seg005/processed/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input features and normalisation param-s (transforms)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T15:59:55.254785Z",
     "start_time": "2021-09-20T15:59:55.149868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Set features window size (#past velocities) to 5\n"
     ]
    }
   ],
   "source": [
    "# Input features\n",
    "window_size = 5\n",
    "print(f'> Set features window size (#past velocities) to {window_size}')\n",
    "\n",
    "# Normalisation\n",
    "# for simulated data (simul params ~ normal distr-s): l0_sim=1.0, l0_HaraMovie: l0=26.32\n",
    "Tnrm = {} # dict of transforms\n",
    "\n",
    "# Default for all simulations:\n",
    "Tnrm['sim'] = T.Compose([Pos2Vec(scale=10*l_0['sim']) , ScaleVelocity(0.5*l_0['single_distr_sims']),\n",
    "                         ScaleTension(t_sd['sim'], shift=t_av['sim']),\n",
    "                         Reshape_x((-1,window_size*2)) ] )\n",
    "\n",
    "# w/o base contractility:\n",
    "Tnrm['single_distr_sims'] = T.Compose([Pos2Vec(scale=10*l_0['single_distr_sims']) , ScaleVelocity(0.5*l_0['single_distr_sims']),\n",
    "                                       ScaleTension(t_sd['single_distr_sims'], shift=t_av['single_distr_sims']),\n",
    "                                       Reshape_x((-1,window_size*2)) ] )\n",
    "\n",
    "# w/ base contractility:\n",
    "Tnrm['unimodal_wbasetens_sims'] = T.Compose([Pos2Vec(scale=10*l_0['unimodal_wbasetens_sims']) , ScaleVelocity(0.5*l_0['unimodal_wbasetens_sims']),\n",
    "                                             ScaleTension(t_sd['unimodal_wbasetens_sims'], shift=t_av['unimodal_wbasetens_sims']),\n",
    "                                             Reshape_x((-1,window_size*2)) ] )\n",
    "\n",
    "# Hara ablation norm\n",
    "Tnrm['abln'] = T.Compose([Pos2Vec(scale=10*l_0['abln']), ScaleVelocity(0.5*l_0['abln']), RecoilAsTension(),\n",
    "                          ScaleTension(t_sd['abln'], shift=t_av['abln']),\n",
    "                          Reshape_x((-1,window_size*2))]) \n",
    "\n",
    "# Hara movie dataset norm\n",
    "Tnrm['hara'] = T.Compose([Pos2Vec(scale=10*l_0['hara']) , ScaleVelocity(0.5*l_0['hara']), Reshape_x((-1,window_size*2))]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T15:59:58.737453Z",
     "start_time": "2021-09-20T15:59:56.647310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n",
      "Processing...\n",
      "Done!\n",
      "Processing...\n",
      "Done!\n",
      "Processing...\n",
      "Done!\n",
      "Processing...\n",
      "Done!\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "hara_abln = HaraAblation('simgnn_data/hara_ablation/', window_size=window_size, transform=Tnrm['abln'], smoothing=True, sma_lag_time=3)\n",
    "hara = HaraMovies('simgnn_data/hara_movies/',window_size=window_size, transform=Tnrm['hara'], smoothing=True, sma_lag_time=4)\n",
    "\n",
    "# Hara movie datasets\n",
    "# seg001 = HaraMovies('simgnn_data/hara_movies_as_sep_datasets/hara_seg001/', window_size=window_size, transform=Tnorm_hara,smoothing=True, sma_lag_time=4)\n",
    "# seg001_raw = HaraMovies('simgnn_data/hara_movies/', window_size=window_size, transform=Tnorm_hara)\n",
    "# seg003 = HaraMovies('simgnn_data/hara_movies_as_sep_datasets/hara_seg003/', window_size=window_size, transform=Tnorm_hara,smoothing=True, sma_lag_time=4)\n",
    "# seg005 = HaraMovies('simgnn_data/hara_movies_as_sep_datasets/hara_seg005/', window_size=window_size, transform=Tnorm_hara,smoothing=True, sma_lag_time=4)\n",
    "\n",
    "# Simulation datasets\n",
    "# rand base contractility\n",
    "sim1 = VertexDynamics('./simgnn_data/unimodal_wbasetens_sims/train/', window_size=window_size,\n",
    "                      transform=Tnrm['unimodal_wbasetens_sims']\n",
    "                     )\n",
    "sim1_val = VertexDynamics('./simgnn_data/unimodal_wbasetens_sims/val/', window_size=window_size,\n",
    "                          transform=Tnrm['unimodal_wbasetens_sims']\n",
    "                         )\n",
    "\n",
    "# w/o base contractility\n",
    "sim2 = VertexDynamics('./simgnn_data/single_distr_sims/train/', window_size=window_size,\n",
    "                      transform=Tnrm['single_distr_sims'])\n",
    "sim2_val = VertexDynamics('./simgnn_data/single_distr_sims/val/', window_size=window_size,\n",
    "                          transform=Tnrm['single_distr_sims']\n",
    "                         )\n",
    "\n",
    "\n",
    "datasets_dict = {'train': sim1,\n",
    "                 'val': sim1_val,\n",
    "                 'val2': sim2_val,\n",
    "                 'hara' : hara,\n",
    "                 'abln': hara_abln}\n",
    "dataset_legend={'train': 'Train(sim1)',\n",
    "                'val': 'Val(sim1)',\n",
    "                'val2': 'Val(sim2)',\n",
    "                'hara': 'Hara',\n",
    "                'abln': 'Recoil'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T16:33:18.750953Z",
     "start_time": "2021-09-20T16:33:18.715032Z"
    }
   },
   "outputs": [],
   "source": [
    "# k = 'val2'\n",
    "# t = 15\n",
    "# print('{}\\n{} : {}\\nsize:{}\\n\\nFrame {}:\\n{}'.format(dataset_legend[k],k,datasets_dict[k],len(datasets_dict[k]),t,datasets_dict[k][0]))\n",
    "# plt.figure(figsize=[2.5,2.5])\n",
    "# nx.draw(to_networkx(datasets_dict[k][t]),pos=dict(enumerate(datasets_dict[k][t].pos.numpy())), node_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Persistence Losses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T16:33:20.994863Z",
     "start_time": "2021-09-20T16:33:20.959105Z"
    }
   },
   "outputs": [],
   "source": [
    "# print('Persistence:')\n",
    "# for k in datasets_dict:\n",
    "#     if k=='abln':print('Ignoring ablation data velocity');continue\n",
    "#     print(f'\\t- {dataset_legend[k]}: {persistence_loss(datasets_dict[k])}')\n",
    "    \n",
    "# print(len(datasets_dict['train']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tension Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T16:33:22.384390Z",
     "start_time": "2021-09-20T16:33:22.338722Z"
    }
   },
   "outputs": [],
   "source": [
    "from simgnn.train import train_model, write_log, load_log, plot_losses\n",
    "from simgnn.train import predict, predict_batch, plot_velocity_predictions, plot_tension_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T17:50:48.905241Z",
     "start_time": "2021-09-23T17:50:47.795863Z"
    }
   },
   "outputs": [],
   "source": [
    "from simgnn.nn import mlp, IndependentBlock, MessageBlock\n",
    "# from simgnn.nn import Message, AggregateUpdate, EdgeUpdate, NodeUpdate, SequentialUpdate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need\n",
    "- Graph Block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T17:51:46.907970Z",
     "start_time": "2021-09-23T17:51:46.873301Z"
    }
   },
   "outputs": [],
   "source": [
    "net = MessageBlock({'node': 10, 'edge': 2}, 8, updt='p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T17:21:55.286437Z",
     "start_time": "2021-09-23T17:21:55.251525Z"
    }
   },
   "outputs": [],
   "source": [
    "# hv, _, he = net(d.x, d.edge_index, d.edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "211.796875px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
