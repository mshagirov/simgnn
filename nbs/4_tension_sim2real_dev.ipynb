{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Network\n",
    "---\n",
    "> Graph neural network model for vertex dynamics and tension prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To-Do**ðŸ‘·ðŸš§\n",
    "- need a func-n w/ a **rollout error**,\n",
    "- convert vel-y error to **position error**, e.g. \"speed\"+\"direction\"(angle/dot product etc.)\n",
    "- *Training loop*:\n",
    "- [ ] Write rollout prediction code.\n",
    "- [ ] Experiments (**save all models** w/ backups; will need to test on the real tensions)<br>\n",
    "*Params*: edge attrib-s, cell layer, arch(skip con-s), input noise (for long term prediction). *Errors*: 1-step, and rollout error (check after training at test time).\n",
    "    1. Experiment with GN arch-s w/ *residual* (w/ skip) and *non-residual* architectures.\n",
    "    1. *Edge directions experiment*: train w/ and w/o edge dir-s, do edge dir-s help to speed up training?\n",
    "    1. *Cell layer*: does having dedicated cell processing layer help to increase the accuracy?\n",
    "    1. For best (resnet or non-resnet) run *number of layers and dim-n sizes*. Try deep nets, how does accuracy change with increasing the depth of the net?\n",
    "    1. If resnet is better: try w/and w/o edge dir-s and \"cell layers\".\n",
    "    1. Number of previous velocities (window size).\n",
    "    1. Rollout experiment 1â€” *input noise*: according to Sanchez-Gonzalez, *et al.* \\[ASG2020\\], Brownian noise improves rollout accuracies (long term accuracy of the whole movie, and/or prediction stability/robustness).\n",
    "    1. Rollout experiment 2â€” *rollout training*: train directly on rollout, i.e. use 5- and 10-step loss instead of a 1-step loss for training (slower training).\n",
    "    1. Optional:\n",
    "        - compare MLP vs CONV layers for message passing.\n",
    "        - try with dynamic graphs (construct graphs on the fly based on relative positions, and use cell edges and cell attrib only for queries on `Y_edge`, `Y_cell`).\n",
    "- [ ] Ablation dataset (*real*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Node-to-Cell Encoding/Pooling Layer**:\n",
    "1. Initiate node-to-cell edge attr-s as (source) node attr-s `x[node2cell_index[0]]`.\n",
    "1. Compute node-to-cell edge attr-s using MLP: `e_n2c = MLP( x[node2cell_index[0]] )`\n",
    "1. Aggregate node-to-cell edge attr-s as cell attr-s : `x_cell = Aggregate(e_n2c)`\n",
    "1. Compute new cell attr-s using (encodes `x_cell` into cell attr-s) : `h_cell = MLP_Cell_encoder( x_cell )`\n",
    "\n",
    "```python\n",
    "n2c_model = mlp(...) # \"message\", just node-wise MLP\n",
    "cell_aggr = Aggregate()\n",
    "cell_enc = mlp(...)\n",
    "\n",
    "e_n2c = n2c_model(data.x)[data.node2cell_index[0]]\n",
    "x_cell = cell_aggr(data.cell_pressures.size(0), data.node2cell_index, e_n2c)\n",
    "h_cell = cell_enc(x_cell)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **Adding noise from M steps**: Sum of M normal rand. var-s results in normal var. w/ variance M and s.t.d.=sqrt(M):\n",
    "```python\n",
    "x = np.random.normal(size=(5,1000))\n",
    "y = x.sum(axis=0)\n",
    "z = np.random.normal(size=(1,1000))*np.sqrt(5)\n",
    "plt.hist(x.ravel(),bins=50,label='x',density=True)\n",
    "plt.hist(y        ,bins=50,label='y',density=True)\n",
    "plt.hist(z.ravel(),bins=50,label='z',density=True,alpha=.5)\n",
    "plt.legend();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T06:39:24.807197Z",
     "start_time": "2021-10-07T06:39:24.784496Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T06:39:26.595302Z",
     "start_time": "2021-10-07T06:39:25.038499Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from os import path\n",
    "import datetime\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "import networkx as nx\n",
    "from simgnn.datautils import load_array, load_graph\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (10,10) # use larger for presentation\n",
    "matplotlib.rcParams['font.size']= 16 # use 14 for presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T06:39:26.626278Z",
     "start_time": "2021-10-07T06:39:26.597544Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from simgnn.datasets import persistence_loss, VertexDynamics, HaraMovies, HaraAblation\n",
    "from simgnn.transforms import Pos2Vec, ScaleVelocity, ScaleTension, ScalePressure, Reshape_x, TransformTension, RecoilAsTension\n",
    "# from torch_geometric.utils import to_undirected as T_undir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T06:39:26.663512Z",
     "start_time": "2021-10-07T06:39:26.629061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaults:\n",
      " |-device: cuda\n",
      " |-dtype : torch.float32\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "dtype = torch.float32\n",
    "print(f'Defaults:\\n |-device: {device}\\n |-dtype : {dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data, Stat-s, Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes for normalisation param-s:\n",
    "\n",
    "- Use `simgnn.transforms` to normalise real--Hara movies and ablation,  and simulated movies. Use same normalisation constants for all simulated movies.\n",
    "- For ablation movies, convert to  HaraMovies length scale (pixels) and use same `l_av` as for HaraMovies dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "|Dataset| Avg. edge len.|Tension/Recoil| Transforms (Normalisation)|Notes|\n",
    "|:---:|---:|---:|:---|:---:|\n",
    "|`single_distr_sims` | 1.08 a.u. | range=[0.00131, 4.26]; sd=0.637; median=0.305; mean=0.578; | `[Pos2Vec(scale=10*1.0)`, `ScaleVelocity(0.5*1.0)`, `ScaleTension(0.634,shift=0.6)]`| synthetic; w/o base tension |\n",
    "|`unimodal_wbasetens_sims` | 0.906 a.u. | range=[0.407, 5.47]; sd=0.729; median=1.48; mean=1.6| `[Pos2Vec(scale=10*1.0)`, `ScaleVelocity(0.5*1.0)`, `ScaleTension(0.634,shift=0.6)]`| synthetic; w/ non-zero base tension |\n",
    "|`HaraMovies`  | 26.32 pixels||`[Pos2Vec(scale=10*26.32)`, `ScaleVelocity(0.5*26.32)]`| tissue movies w/o force data|\n",
    "|`HaraAblation`|$26.32\\cdot\\frac{0.4}{0.21}$ pixels| range=[0.0239, 2.61]; sd=0.651; median=0.673; mean=0.826 | `[Pos2Vec(scale=10*50.13), ScaleVelocity(0.5*50.13)]`| short movies w/ recoil data; frame rate:dataset contains vertex positions only from every 10th frame |\n",
    "\n",
    "- Logarithm of tension: ln(Tension/Recoil {raw values})\n",
    "```\n",
    "Single_distr_sims:        range: [-6.63 1.45]; s.d.: 1.22 || median: -1.19; mean: -1.2;\n",
    "Unimodal_wbasetens_sims : range: [-0.9  1.7]; s.d.: 0.451 || median: 0.39; mean: 0.371;\n",
    "Hara ablation:            range: [-3.73 0.958]; s.d.:0.962 || median:-0.396; mean:-0.547;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- **Normalisation param-s**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T06:39:29.388841Z",
     "start_time": "2021-10-07T06:39:29.356396Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data normalisation param-s : sim=\"default\" for simulation data\n",
    "# Average edge lengths\n",
    "l_0 = {'sim':1.0, 'single_distr_sims':1.1, 'unimodal_wbasetens_sims':0.91, 'hara':26.32, 'abln':50.13}\n",
    "\n",
    "# Stat-s for raw tension/recoil values\n",
    "t_av = {'sim':0.6, 'single_distr_sims': 0.58, 'unimodal_wbasetens_sims': 1.6, 'abln': 0.83}\n",
    "t_sd = {'sim':0.63, 'single_distr_sims': 0.64, 'unimodal_wbasetens_sims':0.73 , 'abln': 0.65}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T04:41:33.599176Z",
     "start_time": "2021-10-07T04:41:32.612753Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm -dr simgnn_data/single_distr_sims/train/processed\n",
    "!rm -dr simgnn_data/single_distr_sims/val/processed\n",
    "!rm -dr simgnn_data/unimodal_wbasetens_sims/train/processed\n",
    "!rm -dr simgnn_data/unimodal_wbasetens_sims/val/processed\n",
    "\n",
    "!rm -dr simgnn_data/hara_movies/processed/\n",
    "!rm -dr simgnn_data/hara_ablation/processed/\n",
    "\n",
    "# !rm -dr simgnn_data/hara_movies_as_sep_datasets/hara_seg001/processed/\n",
    "# !rm -dr simgnn_data/hara_movies_as_sep_datasets/hara_seg003/processed/\n",
    "# !rm -dr simgnn_data/hara_movies_as_sep_datasets/hara_seg005/processed/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input features and normalisation param-s (transforms)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T06:39:33.329647Z",
     "start_time": "2021-10-07T06:39:33.226564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Set features window size (#past velocities) to 5\n"
     ]
    }
   ],
   "source": [
    "# Input features\n",
    "window_size = 5\n",
    "print(f'> Set features window size (#past velocities) to {window_size}')\n",
    "\n",
    "# Normalisation\n",
    "# for simulated data (simul params ~ normal distr-s): l0_sim=1.0, l0_HaraMovie: l0=26.32\n",
    "Tnrm = {} # dict of transforms\n",
    "\n",
    "# Default for all simulations:\n",
    "Tnrm['sim'] = T.Compose([Pos2Vec(scale=10*l_0['sim']) , ScaleVelocity(0.5*l_0['single_distr_sims']),\n",
    "                         ScaleTension(t_sd['sim'], shift=t_av['sim']),\n",
    "                         Reshape_x((-1,window_size*2)) ] )\n",
    "\n",
    "# w/o base contractility:\n",
    "Tnrm['single_distr_sims'] = T.Compose([Pos2Vec(scale=10*l_0['single_distr_sims']) , ScaleVelocity(0.5*l_0['single_distr_sims']),\n",
    "                                       ScaleTension(t_sd['single_distr_sims'], shift=t_av['single_distr_sims']),\n",
    "                                       Reshape_x((-1,window_size*2)) ] )\n",
    "\n",
    "# w/ base contractility:\n",
    "Tnrm['unimodal_wbasetens_sims'] = T.Compose([Pos2Vec(scale=10*l_0['unimodal_wbasetens_sims']) , ScaleVelocity(0.5*l_0['unimodal_wbasetens_sims']),\n",
    "                                             ScaleTension(t_sd['unimodal_wbasetens_sims'], shift=t_av['unimodal_wbasetens_sims']),\n",
    "                                             Reshape_x((-1,window_size*2)) ] )\n",
    "\n",
    "# Hara ablation norm\n",
    "Tnrm['abln'] = T.Compose([Pos2Vec(scale=10*l_0['abln']), ScaleVelocity(0.5*l_0['abln']), RecoilAsTension(),\n",
    "                          ScaleTension(t_sd['abln'], shift=t_av['abln']),\n",
    "                          Reshape_x((-1,window_size*2))]) \n",
    "\n",
    "# Hara movie dataset norm\n",
    "Tnrm['hara'] = T.Compose([Pos2Vec(scale=10*l_0['hara']) , ScaleVelocity(0.5*l_0['hara']), Reshape_x((-1,window_size*2))]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T06:39:34.117827Z",
     "start_time": "2021-10-07T06:39:34.045554Z"
    }
   },
   "outputs": [],
   "source": [
    "hara_abln = HaraAblation('simgnn_data/hara_ablation/', window_size=window_size, transform=Tnrm['abln'], smoothing=True, sma_lag_time=3)\n",
    "hara = HaraMovies('simgnn_data/hara_movies/',window_size=window_size, transform=Tnrm['hara'], smoothing=True, sma_lag_time=4)\n",
    "\n",
    "# Hara movie datasets\n",
    "# seg001 = HaraMovies('simgnn_data/hara_movies_as_sep_datasets/hara_seg001/', window_size=window_size, transform=Tnorm_hara,smoothing=True, sma_lag_time=4)\n",
    "# seg001_raw = HaraMovies('simgnn_data/hara_movies/', window_size=window_size, transform=Tnorm_hara)\n",
    "# seg003 = HaraMovies('simgnn_data/hara_movies_as_sep_datasets/hara_seg003/', window_size=window_size, transform=Tnorm_hara,smoothing=True, sma_lag_time=4)\n",
    "# seg005 = HaraMovies('simgnn_data/hara_movies_as_sep_datasets/hara_seg005/', window_size=window_size, transform=Tnorm_hara,smoothing=True, sma_lag_time=4)\n",
    "\n",
    "# Simulation datasets\n",
    "# rand base contractility\n",
    "sim1 = VertexDynamics('./simgnn_data/unimodal_wbasetens_sims/train/', window_size=window_size,\n",
    "                      transform=Tnrm['sim']\n",
    "#                       transform=Tnrm['unimodal_wbasetens_sims']\n",
    "                     )\n",
    "sim1_val = VertexDynamics('./simgnn_data/unimodal_wbasetens_sims/val/', window_size=window_size,\n",
    "                          transform=Tnrm['sim']\n",
    "#                           transform=Tnrm['unimodal_wbasetens_sims']\n",
    "                         )\n",
    "\n",
    "# w/o base contractility\n",
    "sim2 = VertexDynamics('./simgnn_data/single_distr_sims/train/', window_size=window_size,\n",
    "                      transform=Tnrm['sim']\n",
    "#                       transform=Tnrm['single_distr_sims']\n",
    "                     )\n",
    "sim2_val = VertexDynamics('./simgnn_data/single_distr_sims/val/', window_size=window_size,\n",
    "                          transform=Tnrm['sim']\n",
    "#                           transform=Tnrm['single_distr_sims']\n",
    "                         )\n",
    "\n",
    "\n",
    "datasets_dict = {'train': sim1 + sim2,\n",
    "                 'val': sim1_val,\n",
    "                 'val2': sim2_val,\n",
    "                 'hara' : hara,\n",
    "                 'abln': hara_abln}\n",
    "dataset_legend={'train': 'Train(sim1+sim2)',\n",
    "                'val': 'Val(sim1)',\n",
    "                'val2': 'Val(sim2)',\n",
    "                'hara': 'Hara',\n",
    "                'abln': 'Recoil'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T06:39:34.589668Z",
     "start_time": "2021-10-07T06:39:34.556697Z"
    }
   },
   "outputs": [],
   "source": [
    "# k = 'val2'\n",
    "# t = 15\n",
    "# print('{}\\n{} : {}\\nsize:{}\\n\\nFrame {}:\\n{}'.format(dataset_legend[k],k,datasets_dict[k],len(datasets_dict[k]),t,datasets_dict[k][0]))\n",
    "# plt.figure(figsize=[2.5,2.5])\n",
    "# nx.draw(to_networkx(datasets_dict[k][t]),pos=dict(enumerate(datasets_dict[k][t].pos.numpy())), node_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Persistence Losses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T06:39:35.234134Z",
     "start_time": "2021-10-07T06:39:35.202061Z"
    }
   },
   "outputs": [],
   "source": [
    "# print('Persistence:')\n",
    "# for k in datasets_dict:\n",
    "#     if k=='abln':print('Ignoring ablation data velocity');continue\n",
    "#     print(f'\\t- {dataset_legend[k]}: {persistence_loss(datasets_dict[k])}')\n",
    "    \n",
    "# print(len(datasets_dict['train']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tension Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-07T08:21:02.184243Z",
     "start_time": "2021-10-07T08:21:02.143521Z"
    }
   },
   "outputs": [],
   "source": [
    "from simgnn.train import train_model, write_log, load_log, plot_losses\n",
    "from simgnn.train import predict, predict_batch, plot_velocity_predictions, plot_tension_prediction\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# from simgnn.nn import mlp, IndependentBlock, MessageBlock, dims_to_dict, Encode_Process_Decode\n",
    "from simgnn.models import construct_simple_gnn, GraphEncoder, GraphDecoder, GraphProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T14:23:14.958823Z",
     "start_time": "2021-09-28T14:23:14.925226Z"
    }
   },
   "outputs": [],
   "source": [
    "# MessageBlock(..., hidden_dims=[16], updt='p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-01T03:55:04.787254Z",
     "start_time": "2021-10-01T03:55:04.730848Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(2)\n",
    "\n",
    "# GNN architecture (if dim is `int` latent dims nodes=edges)\n",
    "# -----\n",
    "latent_dims = 128\n",
    "\n",
    "encoder_kwrgs    = {'hidden_dims':[64]}\n",
    "\n",
    "processor_kwargs = {'n_blocks': 20,\n",
    "                    'block_type': 'message',\n",
    "                    'is_residual': True,\n",
    "                    'seq': 'n',\n",
    "                    'norm_type': 'ln',\n",
    "                    'block_p': 0.1,  # block dropout (last layer)\n",
    "                    'dropout_p': 0, # dropout for hidden layers (if any)\n",
    "                    'hidden_dims':[]\n",
    "                   }\n",
    "\n",
    "decoder_kwargs   = {'hidden_dims':[64, 32, 8]}\n",
    "\n",
    "net = construct_simple_gnn(OrderedDict([('node', window_size*2), ('edge', 2)]), latent_dims, OrderedDict([('node', 2), ('edge', 1)]),\n",
    "                           encoder_kwrgs=encoder_kwrgs, processor_kwargs=processor_kwargs, decoder_kwargs=decoder_kwargs).to(device)\n",
    "\n",
    "# Training param-s & Data loaders:\n",
    "# -----\n",
    "num_epochs = 100\n",
    "lr = 0.0001\n",
    "bs = 2\n",
    "loss_func = torch.nn.MSELoss(reduction='mean')\n",
    "# loss_func = torch.nn.L1Loss(reduction='mean')\n",
    "\n",
    "opt = torch.optim.Adam(net.parameters(), lr=lr) # SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
    "scheduler = None # torch.optim.lr_scheduler.ReduceLROnPlateau(opt, 'min')\n",
    "\n",
    "# in order to track the batch id for var-s add its key to \"follow_batch\":\n",
    "loaders = {'train' : DataLoader(datasets_dict['train'], batch_size=bs, shuffle=True),\n",
    "           'val':    DataLoader(datasets_dict['val'],  batch_size=bs),\n",
    "           'val2':   DataLoader(datasets_dict['val2'], batch_size=bs),\n",
    "#            'abln':   DataLoader(datasets_dict['abln'], batch_size=bs),\n",
    "#            'hara':   DataLoader(datasets_dict['hara'], batch_size=bs)\n",
    "          }\n",
    "# this tracks batch id for \"cell_pressures_batch\" and \"edge_index_batch\" in addition to node batch ids\n",
    "\n",
    "model_states = list(loaders.keys())\n",
    "use_force_loss = {'train':[True,False], \n",
    "                  'val':[True, False],\n",
    "                  'val2':[True,False] ,\n",
    "                  'abln':[True,False] ,\n",
    "                  'hara' : [False, False]\n",
    "                 }# [tension, pressure]\n",
    "\n",
    "return_best = False\n",
    "\n",
    "model_name = datetime.datetime.now().strftime('%d%m%Y_%H%M%S')+f'_{net.__class__.__name__}_dp{processor_kwargs[\"dropout_p\"]}'+\\\n",
    "             f'blcdp{processor_kwargs[\"block_p\"]}bs{bs}w{window_size}{\"best\" if return_best else \"\"}'\n",
    "save_dir = './simgnn_data/saved_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12102021_184818_Encode_Process_Decode_dp0blcdp0.1bs2w5\n",
      "---\n",
      "Training param-s: #epochs=100, metric=MSELoss(), batch_size=2, optim=Adam, sch-r=none, return_best=False\n",
      "---\n",
      "Epoch 0/99: train_loss_tot=   1.779; train_loss_y=0.008583; train_loss_T=   1.769; |val_loss_tot=    1.72; |val2_loss_tot=   1.811; |18s\n",
      "Epoch 1/99: train_loss_tot=   1.479; train_loss_y=  0.0071; train_loss_T=   1.471; |val_loss_tot=   1.346; |val2_loss_tot=   1.161; |36s\n",
      "Epoch 2/99: train_loss_tot=   1.281; train_loss_y=0.004967; train_loss_T=   1.276; |val_loss_tot=   1.259; |val2_loss_tot=  0.9951; |53s\n",
      "Epoch 3/99: train_loss_tot=   1.079; train_loss_y=0.004101; train_loss_T=   1.075; |val_loss_tot=   1.082; |val2_loss_tot=  0.7549; |70s\n",
      "Epoch 4/99: train_loss_tot=  0.9618; train_loss_y=0.003873; train_loss_T=  0.9575; |val_loss_tot=  0.6683; |val2_loss_tot=   1.014; |87s\n",
      "Epoch 5/99: train_loss_tot=  0.8259; train_loss_y=0.003453; train_loss_T=   0.822; |val_loss_tot=  0.6099; |val2_loss_tot=   1.213; |105s\n",
      "Epoch 6/99: train_loss_tot=  0.7908; train_loss_y=0.003593; train_loss_T=  0.7867; |val_loss_tot=  0.8488; |val2_loss_tot=  0.4977; |123s\n",
      "Epoch 7/99: "
     ]
    }
   ],
   "source": [
    "print(model_name,end='\\n---\\n')\n",
    "\n",
    "net, train_log = train_model(net, loaders, opt, num_epochs=num_epochs, scheduler=scheduler, device=device, model_states = model_states,\n",
    "                               loss_func = loss_func, use_force_loss=use_force_loss, return_best=return_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "plot_losses(train_log, loaders, dataset_legend)\n",
    "# plot_losses(train_log_ln_bs32, loaders, dataset_legend, marker='s', ms=15, linestyle='-', fillstyle='none', legend_prefix='[LN:bs=32] ', figsize=None)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute predictions and plot them\n",
    "# vel-y, tens, pres\n",
    "pred_out, tgt_out, sample_losses = predict_batch(net,\n",
    "                                                 {\n",
    "#                                                   'train' : DataLoader(datasets_dict['train'], batch_size=bs),\n",
    "                                                  'val':    DataLoader(datasets_dict['val'],  batch_size=bs),\n",
    "                                                  'val2':   DataLoader(datasets_dict['val2'], batch_size=bs),\n",
    "                                                  'abln':   DataLoader(datasets_dict['abln'], batch_size=bs),\n",
    "                                                  'hara':   DataLoader(datasets_dict['hara'], batch_size=bs)},\n",
    "                                                 loss_func=torch.nn.MSELoss(reduction='mean'),\n",
    "                                                 use_force_loss = use_force_loss, return_losses = True, device=device)\n",
    "print(''.join([f'- {k} ={sample_losses[k]};\\n' for k in sample_losses]))\n",
    "\n",
    "plot_velocity_predictions(pred_out[0], tgt_out[0], dataset_legend)\n",
    "\n",
    "plot_tension_prediction(pred_out[1], tgt_out[1], dataset_legend,figsize=[15,15],nrows=2,ncols=2)\n",
    "\n",
    "if 'abln' in tgt_out[1]:\n",
    "    y_gt = torch.cat(tgt_out[1]['abln'],axis=0).numpy()\n",
    "    y_pred = torch.cat(pred_out[1]['abln'],axis=0).numpy()[~np.isnan(y_gt)]\n",
    "    print(f'R,p={pearsonr(y_gt[~np.isnan(y_gt)], y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "211.796875px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
